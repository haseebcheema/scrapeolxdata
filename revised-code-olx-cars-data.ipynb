{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054961dd",
   "metadata": {},
   "source": [
    "# Scraping olx cars data by using requests and beautiful soup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4bd5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages/modules/libraries\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ba373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the data\n",
    "def get_the_data(my_dict, new_soup):\n",
    "    \n",
    "    # getting the data from html content\n",
    "    new_soup = new_soup.find_all('div', attrs={'class':'b44ca0b3'})\n",
    "    \n",
    "    # getting the span elements data    \n",
    "    sublists = []\n",
    "    for d in new_soup:\n",
    "        sublists.append(d.find_all('span'))\n",
    "    \n",
    "    # iterating apn elements lists    \n",
    "    for sublist in sublists:\n",
    "        \n",
    "        # adding key\n",
    "        key = sublist[0].text.strip()\n",
    "        \n",
    "        # adding value\n",
    "        value = sublist[1].text.strip()\n",
    "        \n",
    "        # initialize the list for the key if not present\n",
    "        if key not in my_dict:\n",
    "            my_dict[key] = []\n",
    "        \n",
    "        # append the value to the list associated with the key\n",
    "        if value is None or value == \"\":\n",
    "            my_dict[key].append(\"\")\n",
    "        else:\n",
    "            my_dict[key].append(value)\n",
    "        \n",
    "    return my_dict\n",
    "\n",
    "# get title function\n",
    "\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find('h1', attrs={'class':'a38b8112'}).string.strip()\n",
    "    except AttributeError:\n",
    "        title = \"\"\n",
    "        \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18e81f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # url of the webpage to be scraped\n",
    "    URL = \"https://www.olx.com.pk/cars_c84\"\n",
    "    \n",
    "    # defining headers to be sent to the webpage\n",
    "    HEADERS = ({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36', 'Accept-Language':'en-US, en;q=0.5'})\n",
    "\n",
    "    # sending http get request to the webpage\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "    \n",
    "    # get the content from the webpage, parse into html and give a structure to it using beautifulsoup\n",
    "    soup = BeautifulSoup(webpage.content, 'html.parser')\n",
    "    \n",
    "    # getting all div tags containing links\n",
    "    links = soup.find_all('div', attrs={'class':'_9bea76df'})\n",
    "    \n",
    "    # getting all anchor tags containing links\n",
    "    final_links = []\n",
    "    for link in links:\n",
    "        final_links.append(link.find('a'))\n",
    "    \n",
    "    # getting all the anchor links in the links_list\n",
    "    links_list = []\n",
    "    for link in final_links:\n",
    "        links_list.append(link.get('href'))  \n",
    "    \n",
    "    # creating en emtpy dictionary to hold the extracted data\n",
    "    my_dict = {\"Title\":[]}\n",
    "    \n",
    "    # getting all the keys to be added in the dictionary\n",
    "    for link in links_list:\n",
    "        \n",
    "        # generating new url\n",
    "        URL = \"https://www.olx.com.pk\" + link\n",
    "        \n",
    "        # again sending get request to this new url\n",
    "        new_webpage = requests.get(URL, headers=HEADERS)\n",
    "        \n",
    "        # get the content from the webpage, parse into html and give a structure to it using beautifulsoup\n",
    "        new_soup = BeautifulSoup(new_webpage.content, 'html.parser')\n",
    "        \n",
    "        # calling the function\n",
    "        get_the_data(my_dict, new_soup)\n",
    "    \n",
    "    # getting links one by one and extracting details from those links\n",
    "    for link in links_list:\n",
    "        \n",
    "        # generating new url\n",
    "        URL = \"https://www.olx.com.pk\" + link\n",
    "        \n",
    "        # again sending get request to this new url\n",
    "        new_webpage = requests.get(URL, headers=HEADERS)\n",
    "        \n",
    "        # get the content from the webpage, parse into html and give a structure to it using beautifulsoup\n",
    "        new_soup = BeautifulSoup(new_webpage.content, 'html.parser')\n",
    "        \n",
    "        # get qll the data values\n",
    "        my_dict[\"Title\"].append(get_title(new_soup))\n",
    "    \n",
    "    # deleting this\n",
    "    del my_dict[\"KM's driven\"]\n",
    "    \n",
    "    # Using pandas to create dataframe and creating csv file!\n",
    "    olx_data = pd.DataFrame.from_dict(my_dict)\n",
    "    olx_data.to_csv(r'C:\\Users\\Haseeb Cheema\\Desktop\\data\\projects\\pythonprojects\\webscraping\\bs4-requests-pandas\\olxcarsdata\\olxcarsdata.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
